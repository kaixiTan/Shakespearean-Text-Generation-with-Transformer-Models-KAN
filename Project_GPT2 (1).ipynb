{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-PquZSBXy8F"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preproccess\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/767project/Shakespeare_data.csv\")\n",
        "dataset = data['PlayerLine']\n",
        "subset_size = int(len(dataset) * 0.10)\n",
        "subset_indices = np.random.choice(range(len(dataset)), size=subset_size, replace=False)\n",
        "subset_dataset = dataset[subset_indices]\n",
        "dataset = subset_dataset\n",
        "corpus = []\n",
        "with strategy.scope():\n",
        "    for line in dataset:\n",
        "        lowercase_line = line.lower()\n",
        "        corpus.append(lowercase_line)\n",
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcYLtF8AYIoj",
        "outputId": "7d3a8254-7c9c-4239-822a-12e137e1bbe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['inventoried, and every particle and utensil',\n",
              " \"many a time hath banish'd norfolk fought\",\n",
              " 'have burst their cerements, why the sepulchre,',\n",
              " 'life, to make thee a double-dealer, which, out of',\n",
              " 'o, do not swear, my lord of buckingham.',\n",
              " 'i, now the voice of the recorded law,',\n",
              " 'indeed my mother! or were you both our mothers,',\n",
              " 'and of our athens, thine and ours, to take',\n",
              " 'he would show most love. when yet he was but',\n",
              " 'disguised like muscovites, in shapeless gear,']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "# convert into token\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "inputs = tokenizer(corpus, max_length=40, padding='max_length', truncation=True, return_tensors='tf')\n",
        "labels = inputs.input_ids\n",
        "labels = tf.where(labels == tokenizer.pad_token_id, -100, labels)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "# train the model\n",
        "batch_size = 8\n",
        "for epoch in range(3):\n",
        "    for i in range(0, len(inputs.input_ids), batch_size):\n",
        "        batch_input_ids = inputs.input_ids[i:i+batch_size]\n",
        "        batch_labels = labels[i:i+batch_size]\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = model(batch_input_ids, labels=batch_labels, training=True)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.numpy()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvwaNsX2Yds7",
        "outputId": "e4ee572d-dfad-459d-e366-d7e725bc49b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: [4.930343]\n",
            "Epoch 2, Loss: [4.178693]\n",
            "Epoch 3, Loss: [3.5174983]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, max_length):\n",
        "    # Load GPT-2\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
        "\n",
        "    # Call the generate function in GPT\n",
        "    generated_text_samples = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length + len(input_ids[0]),\n",
        "        num_return_sequences=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        repetition_penalty=1.5,\n",
        "        top_p=0.92,\n",
        "        temperature=0.85,\n",
        "        do_sample=True,\n",
        "        top_k=0\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_texts = []\n",
        "    for i, beam in enumerate(generated_text_samples):\n",
        "        decoded_text = tokenizer.decode(beam, skip_special_tokens=True)\n",
        "        generated_texts.append(decoded_text)\n",
        "        print(f\"Generated Text {i+1}: {decoded_text}\")\n",
        "\n",
        "    return generated_texts\n",
        "\n",
        "# Given the prompt and Length\n",
        "prompt_text = \"long live the king\"\n",
        "desired_length = 10\n",
        "generated_texts = generate_text(prompt_text, desired_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5KrNrPzQeAJ",
        "outputId": "0029b2fd-f206-473d-a878-738e18780e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text 1: long live the king, as he laught them.\"\n",
            "\"Does\n",
            "Generated Text 2: long live the king. He says, \"The King of Queens is\n",
            "Generated Text 3: long live the king's standard, he is now forced to make a\n",
            "Generated Text 4: long live the king's life.\n",
            "When it comes to tradition,\n",
            "Generated Text 5: long live the king is mighty.\n",
            "With those two forces he will\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"Dream to be true\"\n",
        "desired_length = 10\n",
        "generated_texts = generate_text(prompt_text, desired_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNbAP5LIuhBc",
        "outputId": "038b6fa8-d702-4ed2-8a5a-247492f28789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text 1: Dream to be true\"? Why can't we have any kind of relationship\n",
            "Generated Text 2: Dream to be true.\n",
            "And after a pretty long night, I\n",
            "Generated Text 3: Dream to be true\" ©Getty Images\n",
            "..\n",
            "Generated Text 4: Dream to be true and break down barriers with clarity.\n",
            "Sadd\n",
            "Generated Text 5: Dream to be true, any time.\n",
            "My name is Ryan Link\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"King of the king\"\n",
        "desired_length = 10\n",
        "generated_texts = generate_text(prompt_text, desired_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smilhg2a0YXd",
        "outputId": "4812c923-7056-4c69-b237-f6c7c8fb2fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text 1: King of the king was born at that time. The princess Anna,\n",
            "Generated Text 2: King of the king's court, and his son; he came to\n",
            "Generated Text 3: King of the king.\n",
            "\n",
            "\n",
            "Diplomacy - four elements from\n",
            "Generated Text 4: King of the king, as King Steven and his siblings married in a\n",
            "Generated Text 5: King of the king\n",
            "\n",
            "\n",
            "Speak to him first and give his\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save to Google Drive\n",
        "model_path = \"/content/drive/My Drive/gpt2_my_model\"\n",
        "model.save_pretrained(model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQLwScfL4sJV",
        "outputId": "3c774b0a-6491-46a7-8652-2b4edec1be3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFGPT2LMHeadModel\n",
        "\n",
        "loaded_model = TFGPT2LMHeadModel.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1-gQvJ05FDe",
        "outputId": "97d62c63-5a92-4e87-e8b2-920528e7a317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at /content/drive/My Drive/gpt2_my_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    }
  ]
}